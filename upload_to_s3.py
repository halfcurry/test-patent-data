import os
import argparse
import s3fs
import time

def upload_parquet_to_s3(local_parquet_path: str, s3_bucket: str, s3_key_prefix: str = None):
    """
    Uploads a local Parquet directory (which usually contains multiple part files
    generated by Dask) to a specified S3 bucket.

    Args:
        local_parquet_path (str): The local path to the Parquet directory,
                                  e.g., 'hupd_processed/2018.parquet'.
        s3_bucket (str): The name of the S3 bucket, e.g., 'your-data-bucket'.
        s3_key_prefix (str, optional): The path within the S3 bucket where the
                                       Parquet files will be stored. If not provided,
                                       it defaults to the base name of local_parquet_path
                                       (e.g., '2018.parquet/').
    """
    if not os.path.isdir(local_parquet_path):
        print(f"Error: The local path '{local_parquet_path}' is not a directory or doesn't exist.")
        print("Please ensure you provide the path to the Parquet folder (e.g., 'hupd_processed/2018.parquet').")
        return

    print("Initializing S3 filesystem client...")
    # s3fs automatically picks up AWS credentials from environment variables,
    # ~/.aws/credentials, or IAM roles if running on AWS compute services.
    fs = s3fs.S3FileSystem()

    # Determine the full S3 target path. This will be treated as a "directory" in S3.
    # The individual Parquet part files will go inside this path.
    if s3_key_prefix is None:
        # If no prefix is given, use the base name of the local folder (e.g., '2018.parquet')
        s3_target_folder = os.path.basename(local_parquet_path)
    else:
        # Ensure the S3 prefix ends with a slash if it's not empty, for proper path construction
        clean_s3_prefix = s3_key_prefix.strip('/') # Remove leading/trailing slashes for easier joining
        s3_target_folder = os.path.join(clean_s3_prefix, os.path.basename(local_parquet_path))

    # Construct the full S3 URL for logging purposes
    s3_url = f"s3://{s3_bucket}/{s3_target_folder}"

    print(f"\n--- Starting Parquet Upload ---")
    print(f"Local source: '{local_parquet_path}'")
    print(f"S3 destination: '{s3_url}'")

    try:
        # Get a list of all individual Parquet part files in the local directory
        local_files = [
            os.path.join(local_parquet_path, f)
            for f in os.listdir(local_parquet_path)
            if os.path.isfile(os.path.join(local_parquet_path, f)) and f.endswith('.parquet')
        ]

        if not local_files:
            print(f"Warning: No .parquet files found in '{local_parquet_path}'. Nothing to upload.")
            return

        total_files = len(local_files)
        uploaded_count = 0
        start_time = time.time()

        for i, local_file in enumerate(local_files):
            file_name = os.path.basename(local_file)
            # Construct the full S3 key for the individual file
            s3_file_key = f"{s3_target_folder}/{file_name}"

            # Display progress on a single line
            current_progress_str = f"Uploading file {i+1}/{total_files}: {file_name}..."
            print(f"{current_progress_str:<80}", end='\r') # Pad to clear previous line content

            fs.put_file(local_file, f"{s3_bucket}/{s3_file_key}") # s3fs put_file takes bucket/key or full URL

            uploaded_count += 1
            elapsed_time = time.time() - start_time
            avg_time_per_file = elapsed_time / uploaded_count if uploaded_count > 0 else 0
            estimated_remaining_time = (total_files - uploaded_count) * avg_time_per_file
            
            # More comprehensive progress update
            progress_line = (
                f"Uploaded {uploaded_count}/{total_files} files ({file_name}). "
                f"Time elapsed: {elapsed_time:.1f}s | Est. remaining: {estimated_remaining_time:.1f}s"
            )
            print(f"{progress_line:<80}", end='\r') # Clear previous line

        # Final newline after the progress bar finishes
        print(f"\nSuccessfully uploaded {total_files} files to S3 destination: {s3_url}")

    except s3fs.S3FileSystemError as s3_err:
        print(f"\nS3 specific error during upload: {s3_err}")
        print("Please check your S3 bucket name, region, and IAM permissions.")
    except Exception as e:
        print(f"\nAn unexpected error occurred during upload: {e}")
        print("Please ensure your AWS credentials are properly configured (e.g., via environment variables or ~/.aws/credentials).")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Uploads a local Parquet directory to an AWS S3 bucket."
    )
    parser.add_argument(
        "local_path",
        type=str,
        help="The local path to the Parquet directory (e.g., 'hupd_processed/2018.parquet')"
    )
    parser.add_argument(
        "s3_bucket",
        type=str,
        help="The name of the S3 bucket to upload to (e.g., 'your-data-bucket')"
    )
    parser.add_argument(
        "--s3-prefix",
        type=str,
        default=None,
        help="Optional: The path within the S3 bucket where the local folder's contents will be placed. "
             "e.g., 'processed_hupd_data/' will result in 's3://bucket/processed_hupd_data/2018.parquet/'"
    )
    
    args = parser.parse_args()

    # Call the main upload function with parsed arguments
    upload_parquet_to_s3(args.local_path, args.s3_bucket, args.s3_prefix)